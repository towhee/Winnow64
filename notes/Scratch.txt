GEMINI FOCUS STACK APPROACH ---------------------------------------------------------

Building an automated, artifact-free focus stacking app in C++ requires a
multi-stage pipeline that transitions from geometric alignment to intelligent focus
detection and multi-scale fusion. To achieve "zero-intervention," you should lean on
Deep Learning for focus-map estimation and Laplacian Pyramids for blending, as these
naturally adapt to different textures and stack depths without needing "sigma" or
"radius" sliders.

1. The Recommended Pipeline To proceed without user intervention, your architecture
should follow this flow:

Stage 1: Pre-processing & Alignment (Registration). Macro stacks often have "focus
breathing" (slight magnification changes). You must align images before fusing.

        Algorithm: Use ORB or SIFT features combined with RANSAC to find the
        homography between frames. For higher precision, follow this with ECC
        (Enhanced Correlation Coefficient) maximization.

Stage 2: Adaptive Focus Map Generation (ML Layer). Instead of simple Laplacian
variance (which requires thresholds), use a pre-trained Multi-Focus Image Fusion
(MFIF) model.

        SOTA Model (2025): SwinMFF or LightMFF. These models use Transformers or
        specialized U-Nets to output a "decision map" (a probability map of which
        pixels are in focus).

        Adaptive Logic: The model treats the stack as a sequence, comparing local
        patches to find the highest-frequency content.

Stage 3: Multi-Scale Fusion (Artifact Reduction). Naive "pixel-picking" causes halos.
You must blend in the frequency domain.

        Algorithm: Laplacian Pyramid Blending. You decompose each image into a
        pyramid of frequency bands. You then blend the pyramids using the decision
        map as a weight at each level.

Tools & LibrariesFor a high-performance C++ implementation, the following stack is
standard for 2025:

        Component/Library/Reason:

        Core VisionOpenCV 4.x / 5.x  Essential for image I/O, registration, and basic
        pyramid functions (buildPyramid).

        ML Inference: LibTorch (PyTorch C++). To run SOTA MFIF models (like SwinMFF)
        directly in C++ with GPU acceleration.

        Parallelism: Intel TBB / OpenMP. Focus stacking is "embarrassingly parallel."
        You can process pyramid levels or alignment across all CPU cores.

        Math/Optimization: Eigen. If you need custom linear algebra for complex
        registration or artifact correction.

        Performance: SIMD (Halide). If you need extreme speed, Halide can
        auto-generate optimized assembly for your fusion loops.

3. Confidence & Artifact Mitigation

        Confidence Level: High. The combination of Deep Learning (for the "where")
        and Pyramid Blending (for the "how") is extremely robust. Traditional
        artifacts like "halos" or "ghosting" usually occur because of hard edges in
        the weight map or poor alignment.

How to fix remaining artifacts:

        Haloing: This is usually caused by the weight map being too "sharp" at depth
        boundaries. By using the Local Laplacian Pyramid approach, you can
        adaptively smooth transitions based on the confidence score of the ML model.

        Ghosting (Moving Objects): If a bug moves during the stack, the ML model
        will see "multi-focus" in two different places. To fix this automatically,
        implement a Consistency Check that penalizes pixels that appear in-focus in
        wildly non-consecutive frames.

        Color Fringing: Use a bilateral filter on the weight map before fusion to
        ensure focus boundaries align with color boundaries (edges).






1. Alignment quality

Goal: sharper fusion, fewer ‚Äúdouble edges‚Äù.

Options:
        1.	Tune FSAlign options
        ‚Ä¢	matchContrast, matchWhiteBalance, lowRes, maxRes, fullResolution.
        ‚Ä¢	Larger maxRes / fullResolution = true ‚Üí better alignment at the cost of speed.
        ‚Ä¢	Smaller lowRes ‚Üí more precise coarse alignment, but slower.
        2.	Reference frame strategy
        ‚Ä¢	Right now: frame 0 is the reference.
        ‚Ä¢	Alternatives:
        ‚Ä¢	Use the ‚Äúmiddle‚Äù frame of the stack as reference.
        ‚Ä¢	Pick the ‚Äúsharpest‚Äù frame (by a quick focus measure) as reference.
        ‚Ä¢	Better reference can reduce accumulated transform error and warping.
        3.	Pre-conditioning
        ‚Ä¢	Optional mild denoise or contrast-stretch on alignment grayscale images can:
        ‚Ä¢	Make feature matching more robust.
        ‚Ä¢	Reduce mis-alignment in low-contrast areas.

‚∏ª

2. Focus-measure / depth-map logic

Goal: get closer to Zerene-like ‚Äúsmart‚Äù decisions about which slice wins.

Even though your current pipeline is mostly ‚ÄúPetteri PMax in a box‚Äù, you now have a modular place for:
        1.	Alternative focus measures
        ‚Ä¢	Wavelet energy (current).
        ‚Ä¢	Local variance, Laplacian, Tenengrad, etc.
        ‚Ä¢	Multi-measure combination (e.g. wavelet at coarse scale + Laplacian at fine scale).
        2.	Depth-map smoothing
        ‚Ä¢	Apply spatial smoothing/regularization to the depth index:
        ‚Ä¢	Median or bilateral filter on the depth map.
        ‚Ä¢	Guided filtering with the fused gray as guidance.
        ‚Ä¢	This reduces noisy depth flicker between neighbouring pixels ‚Üí smoother fusion, fewer micro-halos.
        3.	Depth constraints (optional)
        ‚Ä¢	If you know stack direction (front‚Üíback), you can:
        ‚Ä¢	Penalize wild depth jumps.
        ‚Ä¢	Encourage locally monotonic or gently varying depth.
        ‚Ä¢	This tends to remove isolated ‚Äúspikes‚Äù and pits in the depth map.
        4.	Depth from Zerene (experimental path)
        ‚Ä¢	You already experimented with using a Zerene depth map as depth_idx.
        ‚Ä¢	Long-term option: a ‚ÄúUse external depth‚Äù mode that:
        ‚Ä¢	Loads a depth_idx-style map.
        ‚Ä¢	Uses your fusion and color pipeline with that depth.

‚∏ª

3. Fusion (PMax) itself

Goal: sharper, more Zerene-like output, fewer halos.
        1.	Merge consistency level
        ‚Ä¢	Your Task_Merge has consistency levels:
        ‚Ä¢	0 = pure max selection.
        ‚Ä¢	1 = subband voting.
        ‚Ä¢	2 = subband + neighbour denoise.
        ‚Ä¢	Increasing this:
        ‚Ä¢	Smooths depth decisions.
        ‚Ä¢	Reduces ‚Äúpepper noise‚Äù and small halos.
        ‚Ä¢	Costs some CPU.
        2.	Wavelet levels / padding
        ‚Ä¢	You now control levels via FSFusionWavelet and padding.
        ‚Ä¢	More levels (deeper pyramid):
        ‚Ä¢	Better separation of fine detail vs large blur.
        ‚Ä¢	Sharper PMax but more sensitive to noise and halos.
        ‚Ä¢	Fewer levels:
        ‚Ä¢	Faster, a bit softer, often fewer halos.
        3.	Post-fusion local contrast
        ‚Ä¢	After inverse wavelet, you can:
        ‚Ä¢	Apply micro-contrast / clarity-style adjustment.
        ‚Ä¢	Apply gentle tone-curve stretching.
        ‚Ä¢	This can push your output visually closer to Zerene‚Äôs PMax ‚Äúbite‚Äù.
        4.	Depth-aware blending between slices
        ‚Ä¢	Instead of hard winner-take-all, you can:
        ‚Ä¢	Blend neighbouring slices in depth where depth indices are close.
        ‚Ä¢	That reduces seams along edges and some halo types.

‚∏ª

4. Halo reduction

Goal: fix the bright/dark fringes around edges in the fused result.

You already have a FocusHalo subsystem in the original Winnow pipeline. For the Petteri path you have options:
        1.	Reuse your existing halo remover
        ‚Ä¢	Run FocusHalo on the PMax output as a post-process.
        ‚Ä¢	This is low-risk: you don‚Äôt touch the Petteri internals at first.
        2.	Depth-guided halo cleanup
        ‚Ä¢	Detect strong local gradient + large depth jumps ‚Üí likely halo region.
        ‚Ä¢	Locally blend:
        ‚Ä¢	Fused result with one of the source slices.
        ‚Ä¢	Or with a more conservative focus measure.
        ‚Ä¢	This targets halos only where depth changes abruptly.
        3.	More conservative PMax (less ‚Äúaggressive‚Äù selection)
        ‚Ä¢	Using consistency level 2 and maybe fewer levels:
        ‚Ä¢	Often reduces halos at the cost of a slightly softer image.
        ‚Ä¢	You can offer a ‚ÄúZerene-like‚Äù vs ‚ÄúAggressive PMax‚Äù preset.

‚∏ª

5. Color handling and bit depth

Goal: best possible color and tonal smoothness, especially if you go to 16-bit output.
        1.	Keep color 16-bit as long as possible
        ‚Ä¢	Aligned color already comes from 8- or 16-bit sources.
        ‚Ä¢	Your current FSFusionReassign:
        ‚Ä¢	Converts 16UC3 ‚Üí 8UC3 for building the color map.
        ‚Ä¢	Option:
        ‚Ä¢	Still build the map in 8-bit space (for logic simplicity).
        ‚Ä¢	Write 16-bit TIFF by scaling the chosen 8-bit color to 16-bit (√ó257).
        ‚Ä¢	This preserves smoother gradients in the final file even if mapping logic is 8-bit.
        2.	Linear vs gamma-encoded color
        ‚Ä¢	If aligned color is in gamma-encoded space (sRGB), PMax can over-emphasize highlights.
        ‚Ä¢	Optional improvement:
        ‚Ä¢	Convert color to a linearized space for reassignment, then re-encode to sRGB at the end.
        3.	White balance and exposure
        ‚Ä¢	FSAlign already has matchWhiteBalance and matchContrast.
        ‚Ä¢	Making sure color is consistent across stack:
        ‚Ä¢	Reduces color flicker in the fused result.
        ‚Ä¢	Helps the reassignment map stay compact and predictable.

‚∏ª

6. Performance and usability

Goal: shorter run times and smoother UX without hurting quality.
        1.	Use in-memory Mats when available
        ‚Ä¢	You are already partway there:
        ‚Ä¢	m_alignedColorMats / m_alignedGrayMats from runAlign().
        ‚Ä¢	When these are populated:
        ‚Ä¢	Skip disk I/O entirely in runFusion().
        ‚Ä¢	For 12√ó20MP stacks that‚Äôs a big quality-of-life win.
        2.	OpenCL toggle
        ‚Ä¢	useOpenCL already gives a big speed-up in forward/inverse wavelet.
        ‚Ä¢	For debugging or older GPUs:
        ‚Ä¢	Allow a ‚ÄúCPU only‚Äù switch.
        ‚Ä¢	For production:
        ‚Ä¢	Default to OpenCL where stable ‚Üí keep current good performance.
        3.	Parallelism at the slice level
        ‚Ä¢	Forward wavelet per slice is embarrassingly parallel:
        ‚Ä¢	You can run multiple slices at once using QtConcurrent or your own thread pool.
        ‚Ä¢	Same for building the color map if you ever split it into tiles.
        4.	Preview vs final mode
        ‚Ä¢	Fast ‚Äúpreview‚Äù:
        ‚Ä¢	Fewer levels.
        ‚Ä¢	Lower consistency.
        ‚Ä¢	Maybe half-resolution input.
        ‚Ä¢	‚ÄúFinal render‚Äù:
        ‚Ä¢	Full levels, consistency 2, full-res.
        ‚Ä¢	This would let you iterate on settings quickly.

‚∏ª

7. Workflow / controls in Winnow

Goal: give you knobs without touching code each time.

Things that map nicely into UI options:
        ‚Ä¢	PMax profile:
        ‚Ä¢	‚ÄúFast‚Äù / ‚ÄúBalanced‚Äù / ‚ÄúMax Quality‚Äù.
        ‚Ä¢	Consistency level: 0 / 1 / 2.
        ‚Ä¢	Use OpenCL: on/off.
        ‚Ä¢	Keep intermediates: on/off (already in Options).
        ‚Ä¢	Output bit depth: 8-bit / 16-bit TIFF.
        ‚Ä¢	Halo reduction: off / mild / strong.


We can start with clean, minimal versions that:
        ‚Ä¢	Use in-memory Mats from runAlign()
        ‚Ä¢	Save only if keepIntermediates==true
        ‚Ä¢	Produce a real depth_index.png preview (you already have reference images)

Then later we enhance:
        ‚Ä¢	Focus metric quality
        ‚Ä¢	Depth smoothing
        ‚Ä¢	Depth confidence
        ‚Ä¢	Depth-aware halo reduction

NOTES AFTER RE-ARCHITECT: (2025-12-10)

2. Depth becomes a FIRST-CLASS citizen

Before:
Fusion also secretly computed its own depth (wavelet merges).
That meant artifacts could originate from either stage, and diagnosing them was hard.

Now:
        ‚Ä¢	Depth is a single, authoritative product of FSDepth.
        ‚Ä¢	Fusion must consume it.
        ‚Ä¢	MultiScale depth = exactly the wavelet-based PMax logic.
        ‚Ä¢	Simple depth = fast, standard scalar-metric argmax.

This makes it trivial to compare or introduce new depth estimators:
        ‚Ä¢	Bilateral-smoothed depth
        ‚Ä¢	Multi-scale confidence maps
        ‚Ä¢	Machine-learning focus measure
        ‚Ä¢	Hybrid depth (blend wavelet + local contrast)
        ‚Ä¢	Depth consistency refinement

The architecture supports them without touching Fusion.

‚∏ª

3. Fusion stage becomes pluggable with no cross-dependencies

Fusion now takes only:
        ‚Ä¢	aligned gray stack
        ‚Ä¢	aligned color stack
        ‚Ä¢	canonical depth map
        ‚Ä¢	method choice (‚ÄúPMax‚Äù, ‚ÄúSimple‚Äù, future methods)

This is perfect for adding:

üîπ PMax2 (your improved PMax)
        ‚Ä¢	Multi-scale soft blending
        ‚Ä¢	Contrast-weighted correction
        ‚Ä¢	Edge-aware halo suppression
        ‚Ä¢	Wavelet residual correction

üîπ DMap Fusion (depth-only selection)
        ‚Ä¢	Already implemented (‚ÄúSimple‚Äù)

üîπ Hybrid fusion
        ‚Ä¢	Use depth to pick ‚Äúblocks‚Äù, then use wavelets within blocks

üîπ Full GPU fusion
        ‚Ä¢	Depth unchanged
        ‚Ä¢	Only wavelet transforms move to GPU

Since depth is fixed and external, fusion methods can be swapped freely.

‚∏ª

4. Performance improvements become easy

Because stages are modular, you can independently optimize:

Alignment

GPU transforms via OpenCL, NEON, or Vulkan
Parallel align of subregions

Focus

Parallel wavelet metrics
SIMD focus measure

Depth

GPU-accelerated FSMerge
Multi-threaded depth refinement
Confidence map generation

Fusion

Reuse wavelet forward transforms
Avoid recomputing padded images
GPU inverse wavelet & color reassignment

All optimizations are local to their stage.

‚∏ª

5. Debugging becomes dramatically easier

You now have canonical artifacts at each stage:
        ‚Ä¢	gray_*.tif
        ‚Ä¢	focus_*.tif
        ‚Ä¢	wavelet debug images
        ‚Ä¢	depth_index.png
        ‚Ä¢	fusion result

If an artifact appears:
        ‚Ä¢	Is the chosen depth too shallow? ‚Üí depth stage
        ‚Ä¢	Is fusion causing halos? ‚Üí fusion stage
        ‚Ä¢	Are focus maps flat? ‚Üí focus stage
        ‚Ä¢	Are aligned frames drifting? ‚Üí align stage

If you‚Äôre ready, the next step I recommend is:

‚ú® Add a confidence map to the MultiScale depth

(which will drastically reduce halo issues)

Or‚Äî

‚ú® Add a PMax2 fusion method

which will use:
        ‚Ä¢	depth edges
        ‚Ä¢	slice local contrast
        ‚Ä¢	multi-level blending
        ‚Ä¢	artifact suppression

-------------------------------------------------------------------------------------

GEMINI:

I am writing a focus stack program using OpenCV and c++. The attached image shows
the edge of a flower petal with artifacts in the out of focus background. Can you
suggest methods to avoid edge artifacts, including tools other than OpenCV.

This is a classic problem in focus stacking, often referred to as "haloing" or "edge
bleeding." It typically occurs at the boundary between a sharp foreground object and
a blurred background because the focus measure algorithm gets confused by the
contrast at the edge, or because the images aren't perfectly aligned.

Here are methods to mitigate these artifacts, ranging from improvements to your current C++/OpenCV pipeline to external tools.

### 1\. Diagnosis: Why is this happening?

Based on the image provided, you are likely facing one (or a combination) of these
three issues:

  * **Hard Masking:** Your algorithm is likely making a binary decision per pixel (e.g., "Image A is sharpest here, take pixel A"). This creates jagged transitions.
  * **Focus Breathing (Scale Misalignment):** As a lens changes focus, the focal length changes slightly, effectively zooming in or out. If your stack isn't corrected for scale, the "sharp" petal in one frame is slightly larger/smaller than the "blurred" petal in the next, creating a ghosting edge.
  * **Mask Spillover:** The high contrast of the petal edge looks like "high focus" to algorithms (like Laplacian), causing the "sharp" mask to extend slightly into the background pixels.

-----

### 2\. OpenCV / C++ Solutions

To fix this within your own code, you need to move from "Hard Selection" to "Soft Blending" and refine your depth map.

#### A. Correct for Focus Breathing (Alignment)

Before stacking, you must align the images, specifically accounting for scale changes (affine or homography), not just translation.

  * **Tool:** `cv::findTransformECC` (Enhanced Correlation Coefficient).
  * **Method:** Use `cv::MOTION_AFFINE` or `cv::MOTION_HOMOGRAPHY`. This ensures the petal edge overlaps perfectly in every slice, reducing the "ghost" halo.

#### B. Refine the Depth Map (The "Guided Filter" Technique)

This is the most effective modern technique for cleaning up edges in focus stacking.

1.  **Compute a noisy depth map:** Calculate your focus metric (e.g., Variance of Laplacian) for each pixel.
2.  **Filter the map:** Instead of just blurring the map (which destroys edge precision), use a **Guided Filter**.
      * **Tool:** `cv::ximgproc::guidedFilter` (part of `opencv_contrib` module).
      * **Logic:** Use the original sharp image as the "guide" and your noisy focus map as the "source." The filter smooths the noise in the background but preserves the sharp edge of the petal because the "guide" image has an edge there.
      * **Result:** A clean, sharp mask that hugs the petal edge perfectly without spilling into the background.

#### C. Laplacian Pyramid Blending

If you are simply taking the maximum sharpness pixel (`max_val`), you will always get artifacts. You should implement Pyramid Blending.

  * **Concept:** Deconstruct the images into different frequency bands (Gaussian/Laplacian pyramids).
  * **Method:**
    1.  Create a mask based on your focus metric (softened/feathered).
    2.  Build a **Laplacian Pyramid** for each source image.
    3.  Build a **Gaussian Pyramid** for the mask.
    4.  Blend the pyramid levels: `Result_Level_i = (ImageA_Level_i * Mask_Level_i) + (ImageB_Level_i * (1 - Mask_Level_i))`.
    5.  Collapse the pyramid (`cv::pyrUp`) to get the final image.
  * **Why it works:** It blends low frequencies (color/tone) smoothly while keeping high frequencies (edges/texture) sharp.

#### D. Morphological Operations

If implementing a full Guided Filter or Pyramid is too heavy, a simpler fix is morphological cleanup on your focus mask.

  * **Dilate:** Expand the "sharp" region slightly to ensure the full edge is included.
  * **Erode:** Trim back noisy speckles in the background.
  * **Gaussian Blur:** Feather the mask edges so the transition isn't binary.

-----

### 3\. Alternative Tools (Non-OpenCV)

If you want to integrate an existing robust solver rather than writing the math from scratch, these are the industry standards.

#### Enfuse (Open Source / CLI)

Enfuse is the engine behind Hugin. It is arguably the best open-source focus stacker available and works on "Exposure Fusion" principles that apply perfectly to focus stacking.

  * **Why use it:** It uses multi-resolution spline blending (Pyramids) by default, which specifically eliminates the artifacts shown in your image.
  * **Usage:** You can call `enfuse` via `system()` in C++.
    ```bash
    enfuse --exposure-weight=0 --saturation-weight=0 --contrast-weight=1 --hard-mask --output=result.tif input_*.tif
    ```
      * `--contrast-weight=1`: Tells it to prioritize sharpness.

#### Helicon Focus / Zerene Stacker (Reference)

These are commercial tools, but understanding their methods helps.

  * **PMax (Pyramid Maximum):** Good for overlapping details but can increase noise (halos).
  * **DMap (Depth Map):** Similar to the "Guided Filter" approach above. It calculates a depth map and then retouches the edges. If your current output looks like "PMax," try moving toward a "DMap" approach.

### Summary Comparison

| Method | Complexity | Effectiveness on Artifacts |
| :--- | :--- | :--- |
| **Simple Max-Pixel** | Low | Low (Creates artifacts) |
| **Mask Morph + Blur** | Medium | Medium (Softens edges) |
| **Guided Filter** | High | **Very High** (Snaps mask to edges) |
| **Pyramid Blending** | High | **Very High** (Seamless transition) |
| **Enfuse (External)** | Low (Integration) | **Very High** |

### Recommended Next Step

Since you are already in C++, I recommend implementing **`cv::ximgproc::guidedFilter`** on your focus mask. It is usually the "magic bullet" for the specific edge-spill artifact shown in your image.

Would you like a C++ code snippet showing how to apply the Guided Filter to a focus map?

BACKGROUND --------------------------------------------------------------------

Why two stages is the correct next step

Stage 1 ‚Äî High-confidence seeding (data-adaptive)

Goal: identify pixels that are almost certainly background or subject.

Examples of safe background seeds:
        ‚Ä¢	Very low max focus and
        ‚Ä¢	Low focus support and
        ‚Ä¢	Stable far depth relative to image statistics

Key point:

These thresholds must be percentile-based, not absolute numbers.

E.g.
        ‚Ä¢	bottom 3‚Äì5% of focus energy
        ‚Ä¢	top 5‚Äì10% of normalized depth
        ‚Ä¢	low local gradient magnitude

This immediately fixes your current failure where:
        ‚Ä¢	background confidence is ‚Äúwashed out‚Äù
        ‚Ä¢	subject interior leaks into background

You should end Stage 1 with:
        ‚Ä¢	BG_SEED mask
        ‚Ä¢	SUBJECT_SEED mask
        ‚Ä¢	everything else = unknown

No feathering, no morphology yet.

‚∏ª

Stage 2 ‚Äî Region grow / graph propagation

Now enforce what thresholding cannot:
        ‚Ä¢	spatial continuity
        ‚Ä¢	edge respect
        ‚Ä¢	texture coherence

Mechanically, this can be:
        ‚Ä¢	flood fill / BFS from seeds
        ‚Ä¢	or graph cut / random walker
        ‚Ä¢	or confidence diffusion constrained by gradients

Cost function uses:
        ‚Ä¢	focus contrast
        ‚Ä¢	depth discontinuity
        ‚Ä¢	image gradients (from fused or mid-stack slice)

This stage answers questions like:

‚ÄúShould this pixel belong with its neighbors?‚Äù

That is impossible to answer correctly with per-pixel scoring alone.

‚∏ª

Why this fits your ‚Äúzero user adjustment‚Äù goal

A seed + grow system can be:
        ‚Ä¢	self-normalizing (percentiles, image stats)
        ‚Ä¢	stack-size invariant
        ‚Ä¢	lens / magnification invariant
        ‚Ä¢	background-type invariant

That is exactly what deterministic pipelines can do well ‚Äî and ML is not strictly required yet.

FSBackground
  ‚îú‚îÄ buildSeeds()
  ‚îú‚îÄ growRegions()
  ‚îî‚îÄ optional edge feather

Answer this: Which regions are certainly background, and how far can we safely expand from them?

New FSBackground Architecture

Stage 1 ‚Äî Seed Detection (High-Confidence Background Only)

Goal:

Identify pixels that are almost certainly background, even if that set is small.

These must be:
        ‚Ä¢	Extremely conservative
        ‚Ä¢	Near-zero false positives
        ‚Ä¢	Allowed to miss a lot (low recall is fine)

Seed Criteria (Data-Adaptive)

Instead of fixed thresholds, use relative statistics:

1. Depth Extremes (Primary)
        ‚Ä¢	Compute depth histogram
        ‚Ä¢	Identify the last depth mode (far plane)
        ‚Ä¢	Seeds = pixels in top 1‚Äì2% depth quantile, and
        ‚Ä¢	Local depth variance is low

This adapts automatically to stack length and subject depth.

2. Focus Absence (Support)
        ‚Ä¢	Compute:
        ‚Ä¢	max focus per pixel
        ‚Ä¢	focus support ratio
        ‚Ä¢	Seeds only if:
        ‚Ä¢	pixel is in bottom 5‚Äì10% of focus energy, relative to image
        ‚Ä¢	AND support ratio is very low

Important: these are percentiles, not fixed values.

3. Color Stability (Optional, weak)
        ‚Ä¢	Very low temporal variance across sl

Output from stage 1: cv::Mat bgSeedMask8;  // CV_8U, 255 = strong background seed

Stage 2 ‚Äî Region Growing (Context-Aware Expansion)

Now we expand from seeds using consistency rules, not thresholds.

Growth Rules (Pixel joins background if‚Ä¶)

A pixel joins background if connected to a seed and:
        1.	Depth Consistency
        ‚Ä¢	Depth index close to seed depth (¬±k slices)
        ‚Ä¢	OR depth gradient is shallow
        2.	Focus Non-Dominance
        ‚Ä¢	Pixel does not have focus significantly higher than neighbors
        ‚Ä¢	Relative comparison, not absolute
        3.	Edge Awareness
        ‚Ä¢	Do not cross strong image gradients
        ‚Ä¢	Do not cross strong focus gradients

STEPS:
1.	Refactor FSBackground
‚Ä¢	Separate seed detection and region grow
‚Ä¢	Remove global bgConfidence computation
2.	Implement conservative seed logic
‚Ä¢	Percentile-based depth + focus
‚Ä¢	Expect sparse output
3.	Implement spatial region growth
‚Ä¢	BFS from seeds
‚Ä¢	Stop at focus/depth edges

This prevents bleeding into subject edges (your current failure mode).


