// Set and unset bit flags example
setWindowFlags(windowFlags() | Qt::WindowStaysOnTopHint);
setWindowFlags(windowFlags() & (~Qt::WindowStaysOnTopHint));

// Qt version control
#if (QT_VERSION < QT_VERSION_CHECK(6, 0, 0))
    qDebug() << "CLASSFUNCTION" << progressLabel->pixmap()->width();
#else
    qDebug() << "CLASSFUNCTION" << progressLabel->pixmap(Qt::ReturnByValueConstant()).width();
#endif

// disable QAction shortcuts
QList<QAction*> actions = findChildren<QAction*>();
for (QAction *a : actions) a->setShortcutContext(Qt::WidgetShortcut);

// enable QAction shortcuts
QList<QAction*> actions = findChildren<QAction*>();
for (QAction *a : actions) a->setShortcutContext(Qt::WindowShortcut);


// delete windows registry in installer
"installscript.js"
...
Component.prototype.createOperations = function()
{
if (installer.value("os") == "win")
{
component.addElevatedOperation("Execute","echo do nothing","UNDOEXECUTE","cmd /C reg delete "HKEY_CURRENT_USER\Software\Company" /f")
...

The echo in the line is necessary, otherwise the installer crashes.
This seems to be a bug in the installer.
It works quite well, now all registry entries are deleted.


// applescript open in photoshop

on openInPhotoshop(filename, isbn)
    tell application "Adobe Photoshop CC 2017"
        activate
        set thisFile to filename as string
        open alias thisFile
        set a to 1
    end tell
end openInPhotoshop

// Create a label with a png file image
pickLabel = new QLabel(this);
pickLabel->setFixedSize(64,64);
pickLabel->setAttribute(Qt::WA_TranslucentBackground);
pickPixmap = new QPixmap(":/images/checkmark.png");
pickLabel->setAlignment(Qt::AlignRight | Qt::AlignBottom);
pickLabel->setVisible(false);


// memory stuff
struct sysinfo sys_info;
totalmem=(qint32)(sys_info.totalram/1048576);
freemem=(qint32)(sys_info.freeram/1048576); // divide by 1024*1024 = 1048576

// macos screen resolution
auto modes = CGDisplayCopyAllDisplayModes(mainDisplayId, nullptr);
auto count = CFArrayGetCount(modes);
CGDisplayModeRef mode;
int wMax, hMax = 0;
for(auto c=count;c--;){
    mode = (CGDisplayModeRef)CFArrayGetValueAtIndex(modes, c);
    auto w = CGDisplayModeGetWidth(mode);
    auto h = CGDisplayModeGetHeight(mode);
    if (w > wMax) wMax = (int)w;
    if (h > hMax) hMax = (int)h;
}
displayHorizontalPixels = wMax;
displayVerticalPixels = hMax;

// return the displayID from a global screen coordinate
CGPoint point;

CGDirectDisplayID getDisplay()
{
const int maxDisplays = 64;                     // 64 should be enough for any system
CGDisplayErr displayStatus;
CGDisplayCount displayCount;                    // Total number of display IDs
CGDirectDisplayID displayIDs[maxDisplays];	// Array of display IDs

displayStatus = CGGetDisplaysWithPoint (point, maxDisplays, displayIDs, &displayCount);
if (displayStatus != kCGErrorSuccess || displayCount!= 1)
{
printf("CGGetDisplaysWithPoint returned error or the wrong number of displays\n");
return NULL;
}
else
return displayIDs[0];
}

// iterate a list
QListWidget *list = new QListidget;
for (const auto& i : list) {
    qDebug() << i;
}


// Regular expression
QRegExp rx("^(Border|Text|Shape|Graphic)$");

if (rx.exactMatch(objectName) == true){
    // do something
}


// QString with arguments
statusBar()->showMessage(tr("Position: (%1,%2) in top level").arg(row).arg(column));
QString newTemplate = QString("New Template %1").arg(newTemplateCount);


// Algorithm to walk a tree structure
void DataModel::walkTree(const QModelIndex &row)
{
    if (this->hasChildren()) {
        for (int i = 0; i < rowCount(row); ++i){
            walkTree(this->index(i,0,row));
        }
    }
}

// Another walk tree
void forEach(QAbstractItemModel* model, QModelIndex parent = QModelIndex()) {
    for (int r = 0; r < model->rowCount(parent); ++r) {
        QModelIndex index = model->index(r, 0, parent);
        QVariant name = model->data(index);
        qDebug() << name;
        // here is your applicable code
        if( model->hasChildren(index) ) {
            forEach(model, index);
        }
    }
}

// convert Magick Image to QPixmap
void ITMagickGraphic::convertImageToPixmap()
{
Blob imageBlob;
image.magick("XPM");
image.write(&imageBlob);
const QByteArray imageData((char*)(imageBlob.data()),
imageBlob.length());
pixmapGraphic.loadFromData(imageData);
}

void ITMagickGraphic::convertPixmapToImage()
{
QByteArray newImageData;
QBuffer imageBuffer(&newImageData);
imageBuffer.open(QIODevice::WriteOnly);
pixmapGraphic.save(&imageBuffer,"JPEG");
Blob newImageBlob(newImageData.data(),newImageData.size());
Image
newImage(newImageBlob,Geometry(pixmapGraphic.width(),pixmapGraphic.height()),"jpeg");

image = newImage;
}

// working with char and uchar examples
char *buffer;
buffer = new char[length];

const char *buf = ba.data();

-----------
Count unique strings in list: (chatGPT)
#include <iostream>
#include <unordered_map>
#include <vector>

int main() {
    std::vector<std::string> my_list;
    // Add 100,000 strings to my_list

    std::unordered_map<std::string, int> count_map;
    for (const auto &str : my_list) {
        count_map[str]++;
    }

    for (const auto &[str, count] : count_map) {
        std::cout << str << ": " << count << std::endl;
    }

    return 0;
}

This example uses an unordered_map to keep track of the count of each unique string in
the list. The map stores the unique strings as keys and the count of each string as
values. The for loop iterates over the list of strings and for each string, increments
the count in the map. Finally, the program loops over the map and prints out the count of
each unique string.

-----------------------
This code first converts the QImage to an NSImage using the toNSImage method, which is
provided by the QtMacExtras module. Then, an instance of NSSharingServicePicker is
created, configured and presented to the user, allowing them to choose from a variety of
sharing options, such as email, message, or social media.

#include <QtGui/QImage>
#include <QtMacExtras/QMacNativeWidget>
#include <AppKit/NSSharingServicePicker.h>

void shareImage(const QImage &image)
{
    // Convert the image to a native NSImage
    QMacNativeWidget nativeWidget;
    NSImage *nsImage = [[NSImage alloc] initWithData:[image.toNSImage().TIFFRepresentation() mutableCopy]];

    // Create an NSSharingServicePicker and present it
    NSSharingServicePicker *picker = [[NSSharingServicePicker alloc] init];
    [picker setDelegate:nil];
    [picker setShowsServicesButton:NO];
    [picker setSourceRect:NSZeroRect];
    [picker setSourceView:nativeWidget.nativeView()];
    [picker shareImage:nsImage];
}

----------------------------------------------------------------------------------------------

FIND SUBJECT AND EYES

Here's a basic algorithm for finding the center of a subject in an image using C++ and the Qt 6 framework:

Load the image using the Qt QImage class. Convert the QImage to a grayscale image using
the QImage's convertToFormat method. Apply a threshold to the grayscale image to convert
it to a binary image, where the pixels are either black or white depending on whether
they are below or above a certain intensity threshold. You can use the QImage's bits
method to access the image data, and manually perform the thresholding. Loop through all
the white pixels in the binary image, summing up the x and y positions of each pixel.
Divide the sum of the x positions by the number of white pixels to find the average x
position, which is the center of the subject in the x-dimension. Divide the sum of the y
positions by the number of white pixels to find the average y position, which is the
center of the subject in the y-dimension. The center of the subject is at the point
(average x position, average y position). Here's some sample code to implement the above
algorithm in Qt 6:

#include <QImage>
#include <QDebug>

int main()
{
    QImage image("subject.jpg");

    // Convert the image to grayscale
    QImage grayImage = image.convertToFormat(QImage::Format_Grayscale8);

    // Apply a threshold to the grayscale image to get a binary image
    QImage binaryImage(grayImage.size(), QImage::Format_Grayscale8);
    uchar *binaryData = binaryImage.bits();
    uchar *grayData = grayImage.bits();
    int imageSize = grayImage.byteCount();
    for (int i = 0; i < imageSize; i++) {
        if (grayData[i] > 128) {
            binaryData[i] = 255;
        } else {
            binaryData[i] = 0;
        }
    }

    // Sum up the x and y positions of all white pixels
    int xSum = 0;
    int ySum = 0;
    int numWhitePixels = 0;
    for (int y = 0; y < binaryImage.height(); y++) {
        for (int x = 0; x < binaryImage.width(); x++) {
            if (binaryImage.pixelColor(x, y).value() == 255) {
                xSum += x;
                ySum += y;
                numWhitePixels++;
            }
        }
    }

    // Find the center of the subject
    int centerX = xSum / numWhitePixels;
    int centerY = ySum / numWhitePixels;
    QPoint center(centerX, centerY);

    qDebug() << "Center of subject:" << center;

    return 0;
}

----------------------------------------------------------------------------------------------

Here's a basic algorithm for finding the eyes of a subject in an image using C++ and the Qt 6 framework:

Load the image using the Qt QImage class.
Convert the QImage to a grayscale image using the QImage's convertToFormat method.
Apply an edge detection filter to the grayscale image to create a binary edge map. One simple approach is to use the Canny edge detection algorithm, which is available in the OpenCV library.
Find the eyes by looking for circular or oval-shaped objects in the binary edge map. This can be done by performing a blob detection or shape detection algorithm, such as the Hough Circle Transform or Hough Ellipse Transform, both of which are available in the OpenCV library.
Once the eyes are found, their centers can be computed and used to determine the position of the eyes in the image.
Here's some sample code to implement the above algorithm using the OpenCV library and the Qt 6 framework:

#include <opencv2/opencv.hpp>
#include <QImage>
#include <QDebug>

using namespace cv;

int main()
{
    // Load the image as a Qt image
    QImage qImage("subject.jpg");

    // Convert the Qt image to an OpenCV Mat
    cv::Mat mat;
    mat = cv::Mat(qImage.height(), qImage.width(), CV_8UC4, (void*)qImage.bits(), qImage.bytesPerLine());

    // Convert the image to grayscale
    cv::Mat gray;
    cvtColor(mat, gray, cv::COLOR_BGR2GRAY);

    // Apply the Canny edge detection algorithm
    cv::Mat edges;
    Canny(gray, edges, 50, 150);

    // Find circles in the edge map using the Hough Circle Transform
    std::vector<Vec3f> circles;
    HoughCircles(edges, circles, HOUGH_GRADIENT, 1, edges.rows / 8, 100, 30, 0, 0);

    // Draw the circles on the original image
    for (const auto& circle : circles) {
        Point center(cvRound(circle[0]), cvRound(circle[1]));
        int radius = cvRound(circle[2]);
        circle(mat, center, radius, Scalar(0, 0, 255), 2, LINE_AA);
    }

    // Convert the OpenCV Mat back to a Qt image

----------------------------------------------------------------------------------------------

how do I use github copilot to help me create a class that accepts a QImage input and
outputs the coordinates for the eye most in focus, based on using qt 6, c++ and opencv

To use GitHub Copilot to help you create a class that accepts a QImage input and outputs
the coordinates for the eye most in focus, based on Qt 6, C++, and OpenCV, you can follow
these steps:

Step 1: Set up the development environment

Install Qt 6, OpenCV, and any other necessary dependencies for your project.

Create a new C++ project in your favorite IDE, such as Qt Creator or Visual Studio, and
configure it to use Qt 6 and OpenCV libraries. Step 2: Define the class

Define a class in your C++ project that will accept a QImage input and output the
coordinates for the eye most in focus. You can give the class a meaningful name, such as
"EyeFocusDetector". Define the necessary member variables and methods in the class. For
example, you may need member variables to store the input QImage, the coordinates of the
eye most in focus, and any other relevant information. You may also need methods to
process the QImage, detect the eyes, and determine the focus. Step 3: Implement the class

Implement the methods of the EyeFocusDetector class using GitHub Copilot to help you with
the code. GitHub Copilot can provide suggestions and autocompletions based on its trained
knowledge of C++, Qt, and OpenCV. Use Qt 6 functions to load the input QImage into a
format that can be processed by OpenCV. For example, you can use
QImage::convertToFormat() to convert the QImage to a format that OpenCV can handle, such
as QImage::Format_RGB888. Use OpenCV functions to process the QImage and detect the eyes.
For example, you can use the cv::CascadeClassifier class to perform Haar cascade-based
face and eye detection, and cv::Mat class to represent and manipulate the image data.
Implement a method that determines the focus of the eyes based on your desired criteria.
For example, you can calculate the sharpness or contrast of the eyes, or use machine
learning techniques to determine the focus. Update the member variables of the
EyeFocusDetector class with the coordinates of the eye most in focus. Step 4: Test and
refine the class

Write test cases to validate the functionality of your EyeFocusDetector class. Use sample
QImage inputs with known eye coordinates and focus levels to verify that your class
produces the correct output. If necessary, refine the implementation of the class based
on the test results and any additional requirements or constraints of your project.


Using GitHub Copilot in combination with your knowledge of Qt, C++, and OpenCV, you can
create a class that accepts a QImage input and outputs the coordinates for the eye most
in focus. Remember to thoroughly test your implementation and refine it as needed to ensure
its correctness and performance in your specific use case.

----------------------------------------------------------------------------------------------

How do I add the opencv library to my qt app.pro file?

To add the OpenCV library to your Qt .pro file, you need to do the following steps:

Step 1: Install OpenCV

First, you need to install OpenCV on your development machine. You
can download the OpenCV library from the official OpenCV website
(https://opencv.org/releases/) and follow the installation instructions provided for your
specific platform (e.g., Windows, macOS, Linux).

Step 2: Add OpenCV library path to .pro file


Once you have installed OpenCV, you need to add the path to the OpenCV library in your Qt
.pro file. Open your .pro file in a text editor, and add the following lines:

.pro file:

# Specify the path to the OpenCV library
OPENCV_DIR = /path/to/your/opencv/library

# Include the OpenCV headers
INCLUDEPATH += $$OPENCV_DIR/include

# Link against the OpenCV library
LIBS += -L$$OPENCV_DIR/lib -lopencv_core -lopencv_imgproc -lopencv_highgui # Add other OpenCV modules as needed

Replace /path/to/your/opencv/library with the actual path to your OpenCV library
installation directory. Make sure to include the appropriate OpenCV modules in the LIBS
line based on your project's requirements. For example, the above lines include the core,
image processing, and highgui modules of OpenCV, but you may need to include additional
modules depending on the functionality you are using.

Step 3: Configure OpenCV in your Qt project

Next, you need to configure your Qt project to use the OpenCV library. Open your Qt
project in your favorite IDE, such as Qt Creator, and go to the project settings or build
settings.

1. Add the OpenCV library path to the "Library Search Paths" or "Library Directories"
setting. This allows the linker to find the OpenCV library during the build process.

2. Add the OpenCV library names (e.g., "opencv_core", "opencv_imgproc", "opencv_highgui")
to the "Libraries" setting. This tells the linker to link against the OpenCV libraries
during the build process.

3. Make sure to set the correct library file extension based on your platform (e.g.,
".dll" for Windows, ".dylib" for macOS, ".so" for Linux).

Step 4: Build and run your Qt project

After adding the OpenCV library to your .pro file and configuring your Qt project, you
can now build and run your Qt project as usual. The OpenCV library will be linked to your
project, and you can use its functions and classes in your code.

Note: It's important to ensure that the version of OpenCV you installed matches the
version of Qt you are using, as there might be compatibility issues between different
versions. Also, make sure to update the library path and names in your .pro file if you
update your OpenCV installation or move it to a different directory.

----------------------------------------------------------------------------------------------

How to create YOLOv7 object detector and include in qt c+++ project

Src: https://www.hackster.io/spehj/deploy-yolov7-to-jetson-nano-for-object-detection-6728c3

To create a YOLOv7 object detector and include it in a Qt C++ project, you can follow these steps:

• First, create a folder for the YOLO project and clone the YOLOv7 repository using Git1.
• Install OpenCV and CUDA on your system1.
• Download the pre-trained weights for YOLOv7 from the official website
  https://github.com/WongKinYiu/yolov7.
• Convert the weights to a format that can be used by OpenCV using the following command:
  python3 models.py --cfg cfg/yolov7.cfg --weights weights/yolov7.pt --output weights/yolov7.weights1.
• Compile the YOLOv7 detector using CMake.
• Create a new Qt C++ project and add the compiled detector as a library.
• Use the library in your project by including its header files and linking it with your project.
